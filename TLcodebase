-----------------------------------------
## SECTION 1: IMPORT AND IMPORT FROM PACKAGES
# Numpy
import numpy as np
#from numpy.random import seed
#seed(440232650)

# Pandas
import pandas as pd

# Matplotlib
import matplotlib.pyplot as plt 

# Seaborn
import seaborn as sns 

# Warnings
import warnings
warnings.filterwarnings('ignore')

# TensorFlow
import tensorflow
#from tensorflow import set_random_seed
#set_random_seed(440232650)

# Random
import os, random, cv2, h5py, requests

# Utils
import np_utils

# Segmentation Models
import segmentation_models as sm
from segmentation_models import Unet
from segmentation_models import get_preprocessing
# loss functions -->> DOUBLE CHECK THIS
from segmentation_models.losses import jaccard_loss
from segmentation_models.losses import dice_loss
from segmentation_models.losses import binary_focal_loss
from segmentation_models.losses import categorical_focal_loss
from segmentation_models.losses import binary_crossentropy
from segmentation_models.losses import categorical_crossentropy
from segmentation_models.losses import bce_dice_loss                    #= binary_crossentropy + dice_loss
from segmentation_models.losses import bce_jaccard_loss                 #= binary_crossentropy + jaccard_loss
from segmentation_models.losses import cce_dice_loss                    #= categorical_crossentropy + dice_loss
from segmentation_models.losses import cce_jaccard_loss                 #= categorical_crossentropy + jaccard_loss
from segmentation_models.losses import binary_focal_dice_loss           #= binary_focal_loss + dice_loss
from segmentation_models.losses import binary_focal_jaccard_loss        #= binary_focal_loss + jaccard_loss
from segmentation_models.losses import categorical_focal_dice_loss      #= categorical_focal_loss + dice_loss
from segmentation_models.losses import categorical_focal_jaccard_loss   #= categorical_focal_loss + jaccard_loss
# performance metrics
from segmentation_models.metrics import iou_score

# Glob
from glob import glob

# Pillow
from PIL import Image

# IO
form io import BytesIO

# SKLearn
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, roc_auc_score, auc, jaccard_score

# Keras
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from keras.layers import Input, Dropout, Flatten, Dense, AveragePooling2D, Conv2D, GlobalAveragePooling2D, BatchNormalization, Flatten, Activation
from keras.optimizers import Adam, SGD, Adagrad, Adadelta
from keras.models import Model
from keras.models import load_model
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, CSVLogger

# Albumentations
from albumentations import (Compose,
    HorizontalFlip,
    ElasticTransform,
    GridDistortion, 
    OpticalDistortion,
    OneOf,
    CLAHE,
    RandomBrightnessContrast,    
    RandomGamma    
)


-----------------------------------------
## SECTION 2: DEFINE VARIABLES 

# GPU
os.environ["CUDA_VISIBLE_DEVICES"] = 1

# Weight initialisation
weight_initialisation = TL   # transfer learning

# Class distinction
class_dict = {0: 'BG', 1:'SB'}

# Model variables
backbone = vgg16
image_size = 256    # call image_size as variable for both h and w 
classes = 1
batch_size = 8
epochs = 100
  # TL specific
encoder_weights = imagenet    #pre-training on ImageNet
encoder_freeze = True    # set all layers of encoder as non-trainable
  # activation - CHOOSE 1
activation = sigmoid    # use sigmoid for binary
activation = softmax
activation = linear
  # batch normalisation - CHOOSE 1
decoder_use_batchnorm = True  # BatchNormalisation layer between Conv2D and Activation layers is used
decoder_use_batchnorm = False # batchnorm not used

# Loss functions - CHOOSE 1
loss = sm.losses.JaccardLoss()
loss = sm.losses.DiceLoss()
loss = sm.losses.BinaryFocalLoss()
loss = sm.losses.CategoricalFocalLoss()
loss = sm.losses.BinaryCELoss()
loss = sm.losses.CategoricalCELoss()
    # Loss name - CHOOSE 1
loss_name = 'JaccardLoss'
loss_name = 'DiceLoss'
loss_name = 'BinaryFocalLoss'
loss_name = 'CategoricalFocalLoss'
loss_name = 'BinaryCELoss'
loss_name = 'CategoricalCELoss'

# Optimiser parameters (LR, beta, momentum)
learning_rate = 0.0001
beta_1 = 0.9
beta_2 = 0.999
momentum = 0.0

# Optimiser - CHOOSE 1
opt = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, amsgrad=False)
opt = SGD(learning_rate=learning_rate, momentum=momentum, nesterov=False)
opt = Adagrad()                # recommended to leave defaults
opt = Adadelta()               # recommends to leave defaults

# Performance
IOU = sm.metrics.IOUScore()
FScore = sm.metrics.FScore()
metrics=[IOU, FScore]

# Augmentations performed? - CHOOSE 1
da = dAug
da = no_dAug


-----------------------------------------
## SECTION 3: DEFINE PATHS

# Training
ytrain_dir = 'Masks/Train'
xtrain_dir = 'Images/Train'

# Validation
yval_dir = 'Masks/Val'
xval_dir = 'Images/Val'

# Test
ytest_dir = 'Masks/Test'
xtest_dir = 'Images/Test'


# Save Path
save_path = f'Unet.{backbone}.{weight_initialisation}.b{batch_size}.e{epochs}.LR{learning_rate}.da{da}

# Path where resulting weights will be saved
weights_path = f'Weights/{save_path}.h5'


-----------------------------------------
## SECTION 4: AUGMENTATIONS
    # adjust as needed

aug = Compose([OneOf([
        ElasticTransform(p=1, alpha=200, sigma=200 * 0.05, alpha_affine=200 * 0.03),
        GridDistortion(p=1, border_mode=0, value=5)], p=0.8),
    CLAHE(p=0.8),
    HorizontalFlip(p=0.5),
    RandomBrightnessContrast(p=0.8),    
    RandomGamma(p=0.8)])


-----------------------------------------
## SECTION 5: DATA GENERATOR FUNCTION - TRAINING + VALIDATION

# img_folder and mask_folder will be replaced with xtrain_dir/xval_dir and ytrain_dir/yval_dir when data generated in Section 7
def data_gen(img_folder, mask_folder, batch_size=batch_size, image_size=image_size, classes=classes):
    c=0
    n=os.listdir(img_folder)
    random.shuffle(n)
    
    while (True):
        img = np.zeros((batch_size, image_size, image_size, 1)).astype('float')
        mask = np.zeros((batch_size, image_size, image_size, classes)).astype('float')
        
        for i in range(c, c+batch_size):
            
            train_img = cv2.imread(os.path.join(img_folder,n[i]), cv2.IMREAD_GRAYSCALE)
            train_mask = cv2.imread(os.path.join(mask_folder,n[i]), cv2.IMREAD_GRAYSCALE)
            
            # comment this out if not augmenting
            augmented = aug(image=train_img, mask=train_mask)
            train_img = augmented['image']
            train_mask = augmented['mask']
            
            train_img = cv2.resize(train_img, (image_size, image_size))/255.      #change if image size changes
            train_img = train_img.reshape([image_size, image_size,1])
            
            img[i-c] = train_img #add to array img[0] img [1] etc
            
            
            train_mask = cv2.resize(train_mask, (image_size, image_size))
            train_mask = train_mask.reshape([image_size, image_size, classes])
            
            mask[i-c] = train_mask
            
        c+=batch_size
        if (c+batch_size>=len(os.listdir(img_folder))):
            c=0
            random.shuffle(n)
        
        yield img, mask
        
        
        
-----------------------------------------        
## SECTION 6: DATA GENERATOR FUNCTION - TESTING
    # Only call when testing

# define batch_size as number of images in test folder (otherwise issuse with shuffling)
def test_gen(img_folder, mask_folder, batch_size=51, image_size=image_size, classes=classes):
    c=0
    n=os.listdir(img_folder)
    n.sort() #always evaluate in the same order 
    #remove shuffling
    while (True):
        img = np.zeros((batch_size, image_size, image_size, 1)).astype('float')
        mask = np.zeros((batch_size, image_size, image_size, classes)).astype('float')
        
        for i in range(c, c+batch_size):
            
            train_img = cv2.imread(os.path.join(img_folder,n[i]), cv2.IMREAD_GRAYSCALE)/255.
            train_img = cv2.resize(train_img, (image_size, image_size))
            train_img = train_img.reshape([image_size, image_size,1])
            
            img[i-c] = train_img #add to array img[0] img [1] etc
            
            train_mask = cv2.imread(os.path.join(mask_folder,n[i]), cv2.IMREAD_GRAYSCALE)
            train_mask = cv2.resize(train_mask, (image_size, image_size))
            train_mask = train_mask.ravel()
            train_mask = to_categorical(train_mask, num_classes=classes)
            train_mask = train_mask.reshape([image_size, image_size, classes])
            
            mask[i-c] = train_mask
            
        c+=batch_size
        if (c+batch_size>=len(os.listdir(img_folder))):
            c=0
        
        yield img, mask


        
-----------------------------------------        
## SECTION 7: GENERATE THE DATA

# Generate training data (known)
train_gen = data_gen(xtrain_dir, ytrain_dir, image_size=image_size, batch_size=batch_size)

# Generate validation data (known)
val_gen = data_gen(xval_dir, yval_dir, image_size=image_size, batch_size=batch_size)

# Predicted generation for testing only -->
    # Generate predicted validation data (predictions)
    # update batch number so it matches house many images there are in the folder 
val_pred_generator = test_gen(xval_dir, yval_dir, image_size=image_size, batch_size=51)

    # Generate test data (predicted)
    # update batch number so it matches house many images there are in the folder 
test_generator = test_gen(xtest_dir, ytest_dir, image_size=image_size, batch_size=51)



-----------------------------------------        
## SECTION 8: DEFINE MODEL CALLBACKS
    # turn on/off as needed 
    # can add verbose=1 if want to visualise progress bar

# Checkpoint - save minimum val_loss found
checkpoint = ModelCheckpoint(weights_path, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')

# CSVLogger - save epoch results with "save_path" naming attached --> potentially bring TrainingLog up to Paths like weights_path
csv_logger = CSVLogger(f'TrainingLog/{save_path}.out', append=True, separator=';')

# EarlyStopping - stop training if val_loss doesn't improve after # epochs (patience)
earlystopping = EarlyStopping(monitor='val_loss', mode="min", patience=5, verbose=1)
                      
# Reduce Learning Rate - reduce learning rate if val_loss stops improving
reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.2, patience=5, min_lr=0.001, verbose=1)


# Collate callbacks 
callbacks_list = [checkpoint, csv_logger, earlystopping, reduce_lr]


-----------------------------------------        
## SECTION 9: CREATE MODEL

base_model = Unet(backbone_name=backbone, encoder_weights=encoder_weights, classes=classes, activation=activation, 
        encoder_freeze=encoder_freeze, decoder_use_batchnorm = decoder_use_batchnorm)

inp = Input(shape=(image_size, image_size, 1))  # need new input layer for 1-channel medical data

11 = Conv2D(3, (1, 1))(inp)   # map single channel to RGB channels

out = base_model(11)

model = Model(inp, out, name=base_model.name)



-----------------------------------------        
## SECTION 10: COMPILE MODEL

model.compile(optimizer=opt, loss=loss, metrics=metrics) 

# Model summary
model.summary()


-----------------------------------------        
## SECTION 11: BEGIN TRAINING

# Define number of training and validation images
no_training_images = len(os.listdir(xtrain_dir))
no_val_images = len(os.listdir(xval_dir))

# Train the model
results = model.fit_generator(train_gen, epochs=epochs, 
    # augmentation doesn't increase # images, so if augmenting, update # for steps_per_epoch to match correct # of images being trained on)
           steps_per_epoch = (no_training_images*1//batch_size),    
           validation_data=val_gen,
           validation_steps=(no_val_images//batch_size), 
           verbose=1, 
           callbacks = callbacks_list)
           
           
      
-----------------------------------------        
## SECTION 12: GET TEST SCORE
# only call when testing

model.evaluate_generator(test_generator, steps=1)


 
