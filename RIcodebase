-----------------------------------------
## SECTION 1: IMPORT AND IMPORT FROM PACKAGES
# Numpy
import numpy as np
from numpy.random import seed
seed(440232650)

# Pandas
import pandas as pd

# Matplotlib
import matplotlib.pyplot as plt 

# Seaborn
import seaborn as sns 

# Warnings
import warnings
warnings.filterwarnings('ignore')

# TensorFlow
import tensorflow
from tensorflow import set_random_seed
set_random_seed(440232650)

# Random
import os, random, cv2, h5py, requests

# Utils
import np_utils

# Segmentation Models
import segmentation_models as sm
from segmentation_models import Unet, get_preprocessing
    # loss functions -->> DOUBLE CHECK THIS
from segmentation_models.losses import jaccard_loss, dice_loss, binary_focal_loss, categorical_focal_loss, binary_crossentropy, categorical_crossentropy, bce_dice_loss, bce_jaccard_loss, cce_dice_loss, cce_jaccard_loss, binary_focal_dice_loss, binary_focal_jaccard_loss, categorical_focal_dice_loss, categorical_focal_jaccard_loss  
    # performance metrics
from segmentation_models.metrics import iou_score

# Glob
from glob import glob

# Pillow
from PIL import Image

# IO
form io import BytesIO

# SKLearn
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, roc_auc_score, auc, jaccard_score

# Keras
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from keras.layers import Input, Dropout, Flatten, Dense, AveragePooling2D, Conv2D, GlobalAveragePooling2D, BatchNormalization, Flatten, Activation
from keras.optimizers import Adam, SGD, Adagrad, Adadelta
from keras.models import Model, load_model
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, CSVLogger

# Albumentations
from albumentations import (Compose,
    HorizontalFlip,
    ElasticTransform,
    GridDistortion, 
    OpticalDistortion,
    OneOf,
    CLAHE,
    RandomBrightnessContrast,    
    RandomGamma    
)


-----------------------------------------
## SECTION 2: DEFINE VARIABLES 

# GPU
os.environ["CUDA_VISIBLE_DEVICES"] = 1

# Weight initialisation
weight_initialisation = RI   # randomly initialised

# Class distinction
class_dict = {0: 'BG', 1:'SB'}

# Model variables
backbone = vgg16
image_size = 256    # call image_size as variable for both h and w 
classes = 1
batch_size = 8
epochs = 100
    # RI specific
encoder_weights = None          # random initialisation
encoder_freeze = False          # let all layers be trained
    # activation - CHOOSE 1
activation = sigmoid    # use sigmoid for binary
activation = softmax
activation = linear
    # batch normalisation - CHOOSE 1
decoder_use_batchnorm = True  # BatchNormalisation layer between Conv2D and Activation layers is used
decoder_use_batchnorm = False # batchnorm not used

# Loss functions - CHOOSE 1
loss = sm.losses.JaccardLoss()      # similar to dice loss 
loss = sm.losses.DiceLoss()         # most commonly used
loss = sm.losses.BinaryFocalLoss()
loss = sm.losses.CategoricalFocalLoss()
loss = sm.losses.BinaryCELoss()
loss = sm.losses.CategoricalCELoss()
    # Loss name - CHOOSE 1
loss_name = 'JaccardLoss'
loss_name = 'DiceLoss'
loss_name = 'BinaryFocalLoss'
loss_name = 'CategoricalFocalLoss'
loss_name = 'BinaryCELoss'
loss_name = 'CategoricalCELoss'

# Optimiser parameters (LR, beta, momentum)
learning_rate = 0.0001
beta_1 = 0.9
beta_2 = 0.999
momentum = 0.0

# Optimiser - CHOOSE 1
opt = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, amsgrad=False)
opt = SGD(learning_rate=learning_rate, momentum=momentum, nesterov=False)
opt = Adagrad()                # recommended to leave defaults
opt = Adadelta()               # recommends to leave defaults

# Performance
        # can specify (threshold=0.5)
IOU = sm.metrics.IOUScore()
FScore = sm.metrics.FScore()    # dice coefficient
metrics=[IOU, FScore]

# Augmentations performed? - CHOOSE 1
da = dAug
da = no_dAug


-----------------------------------------
## SECTION 3: DEFINE PATHS

# Training
ytrain_dir = 'Masks/Train'
xtrain_dir = 'Images/Train'

# Validation
yval_dir = 'Masks/Val'
xval_dir = 'Images/Val'

# Test
ytest_dir = 'Masks/Test'
xtest_dir = 'Images/Test'


# Save Path
save_path = f'Unet.{backbone}.{weight_initialisation}.b{batch_size}.e{epochs}.LR{learning_rate}.{da}

# Path where resulting weights will be saved
weights_path = f'Weights/{save_path}.h5'




-----------------------------------------
## SECTION 4: AUGMENTATIONS
    # adjust as needed

aug = Compose([OneOf([
        ElasticTransform(p=1, alpha=200, sigma=200 * 0.05, alpha_affine=200 * 0.03),
        GridDistortion(p=1, border_mode=0, value=5)], p=0.8),
    CLAHE(p=0.8),
    HorizontalFlip(p=0.5),
    RandomBrightnessContrast(p=0.8),    
    RandomGamma(p=0.8)])


-----------------------------------------
## SECTION 5: DATA GENERATOR FUNCTION - TRAINING + VALIDATION

# img_folder and mask_folder will be replaced with xtrain_dir/xval_dir and ytrain_dir/yval_dir when data generated in Section 7
def data_gen(img_folder, mask_folder, batch_size=batch_size, image_size=image_size, classes=classes):
    c=0
    n=os.listdir(img_folder)
    random.shuffle(n)
    
    while (True):
        img = np.zeros((batch_size, image_size, image_size, 1)).astype('float')
        mask = np.zeros((batch_size, image_size, image_size, classes)).astype('float')
        
        for i in range(c, c+batch_size):
            
            train_img = cv2.imread(os.path.join(img_folder,n[i]), cv2.IMREAD_GRAYSCALE)
            train_mask = cv2.imread(os.path.join(mask_folder,n[i]), cv2.IMREAD_GRAYSCALE)
            
            # comment this out if not augmenting
            augmented = aug(image=train_img, mask=train_mask)
            train_img = augmented['image']
            train_mask = augmented['mask']
            
            train_img = cv2.resize(train_img, (image_size, image_size))/255.      #change if image size changes
            train_img = train_img.reshape([image_size, image_size,1])
            
            img[i-c] = train_img #add to array img[0] img [1] etc
            
            
            train_mask = cv2.resize(train_mask, (image_size, image_size))
            train_mask = train_mask.reshape([image_size, image_size, classes])
            
            mask[i-c] = train_mask
            
        c+=batch_size
        if (c+batch_size>=len(os.listdir(img_folder))):
            c=0
            random.shuffle(n)
        
        yield img, mask
        
        
        
-----------------------------------------        
## SECTION 6: DATA GENERATOR FUNCTION - TESTING
    # Only call when testing

# define batch_size as number of images in test folder (otherwise issuse with shuffling)
def test_gen(img_folder, mask_folder, batch_size=51, image_size=image_size, classes=classes):
    c=0
    n=os.listdir(img_folder)
    n.sort() #always evaluate in the same order 
    #remove shuffling
    while (True):
        img = np.zeros((batch_size, image_size, image_size, 1)).astype('float')
        mask = np.zeros((batch_size, image_size, image_size, classes)).astype('float')
        
        for i in range(c, c+batch_size):
            
            train_img = cv2.imread(os.path.join(img_folder,n[i]), cv2.IMREAD_GRAYSCALE)/255.
            train_img = cv2.resize(train_img, (image_size, image_size))
            train_img = train_img.reshape([image_size, image_size,1])
            
            img[i-c] = train_img #add to array img[0] img [1] etc
            
            train_mask = cv2.imread(os.path.join(mask_folder,n[i]), cv2.IMREAD_GRAYSCALE)
            train_mask = cv2.resize(train_mask, (image_size, image_size))
            train_mask = train_mask.ravel()
            train_mask = to_categorical(train_mask, num_classes=classes)
            train_mask = train_mask.reshape([image_size, image_size, classes])
            
            mask[i-c] = train_mask
            
        c+=batch_size
        if (c+batch_size>=len(os.listdir(img_folder))):
            c=0
        
        yield img, mask


        
-----------------------------------------        
## SECTION 7: GENERATE THE DATA

# Generate training data (known)
train_gen = data_gen(xtrain_dir, ytrain_dir, image_size=image_size, batch_size=batch_size)

# Generate validation data (known)
val_gen = data_gen(xval_dir, yval_dir, image_size=image_size, batch_size=batch_size)

# Predicted generation for testing only -->
    # Generate predicted validation data (predictions)
    # update batch number so it matches house many images there are in the folder 
val_pred_generator = test_gen(xval_dir, yval_dir, image_size=image_size, batch_size=51)

    # Generate test data (predicted)
    # update batch number so it matches house many images there are in the folder 
test_generator = test_gen(xtest_dir, ytest_dir, image_size=image_size, batch_size=51)



-----------------------------------------        
## SECTION 8: DEFINE MODEL CALLBACKS
    # turn on/off as needed 
    # can add verbose=1 if want to visualise progress bar

# Checkpoint - save minimum val_loss found
checkpoint = ModelCheckpoint(weights_path, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')

# CSVLogger - save epoch results with "save_path" naming attached --> potentially bring TrainingLog up to Paths like weights_path
csv_logger = CSVLogger(f'TrainingLog/{save_path}.out', append=True, separator=';')

# EarlyStopping - stop training if val_loss doesn't improve after # epochs (patience)
earlystopping = EarlyStopping(monitor='val_loss', mode="min", patience=5, verbose=1)
                      
# Reduce Learning Rate - reduce learning rate if val_loss stops improving
reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.2, patience=5, min_lr=0.001, verbose=1)


# Collate callbacks 
callbacks_list = [checkpoint, csv_logger, earlystopping, reduce_lr]


-----------------------------------------        
## SECTION 9: CREATE MODEL

model = Unet(backbone_name=backbone, encoder_weights=encoder_weights, classes=classes, activation=activation, 
        encoder_freeze=encoder_freeze, decoder_use_batchnorm = decoder_use_batchnorm, input_shape=(image_size, image_size, 1))


-----------------------------------------        
## SECTION 10: COMPILE MODEL

model.compile(optimizer=opt, loss=loss, metrics=metrics) 

# Model summary
model.summary()


-----------------------------------------        
## SECTION 11: BEGIN TRAINING

# Define number of training and validation images
no_training_images = len(os.listdir(xtrain_dir))
no_val_images = len(os.listdir(xval_dir))

# Train the model
results = model.fit_generator(train_gen, epochs=epochs, 
    # augmentation doesn't increase # images, so if augmenting, update # for steps_per_epoch to match correct # of images being trained on)
           steps_per_epoch = (no_training_images*1//batch_size),    
           validation_data=val_gen,
           validation_steps=(no_val_images//batch_size), 
           verbose=1, 
           callbacks = callbacks_list)


-----------------------------------------        
## SECTION 12: GET TEST SCORE
# only call when testing

model.evaluate_generator(test_generator, steps=1)




-----------------------------------------        
## SECTION 13: PLOT TRAINING RESULTS

# Define TrainingLog path
TrainingLog_path = f'TrainLog/{save_path}.out'

# Read the TrainingLog data for plotting
history = pd.read_csv(TrainingLog_path, sep=';')

# Create function to plot training and validation IOU score and loss values
    # to save plot figures: save = True, name = save_path
def plot_history(history, save=False, name=None):
    fig, (ax1, ax2) = plt.subplots(ncols=2)
    ax1.plot(history['iou_score'])
    ax1.plot(history['val_iou_score'])
    ax1.title.set_text('Model Accuracy')
    ax1.set_ylabel('Jaccard Score')
    ax1.set_xlabel('Epoch')
    ax1.legend(['Train', 'Validation'], loc='lower right')
    
    # Plot training & validation loss values
    ax2.plot(history['loss'])
    ax2.plot(history['val_loss'])
    ax2.title.set_text('Model loss')
    ax2.set_ylabel('Jaccard Loss')
    ax2.set_xlabel('Epoch')
    ax2.legend(['Train', 'Validation'], loc='upper right')
    fig.tight_layout()
    fig.show()
    if save==False:
        return
    else:
        fig.savefig(f'Figures/{name}.png')      
    return

# Show the plots
plot_history(history, save = True, name = save_path)





